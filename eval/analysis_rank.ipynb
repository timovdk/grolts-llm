{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Set the style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Set general plot parameters for Overleaf (1-column A4)\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"figure.figsize\": (3.3, 2.5),  # inches, ~1-column width\n",
    "        \"axes.titlesize\": 16,\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12,\n",
    "        \"legend.fontsize\": 14,\n",
    "        \"pdf.fonttype\": 42,  # vector fonts\n",
    "        \"ps.fonttype\": 42,\n",
    "    }\n",
    ")\n",
    "\n",
    "PATH = \"./outputs\"\n",
    "SUBFOLDERS = [\"wellbeing\", \"achievement\", \"delinquency\", \"ptsd\"]\n",
    "LABELS = [\n",
    "    \"gpt-5-mini\",\n",
    "    \"Phi-4\",\n",
    "    \"Qwen-3-30B\",\n",
    "    \"Llama-3.3-70B\",\n",
    "    \"Magistral-Small\",\n",
    "    \"Qwen-Next-80B\",\n",
    "]\n",
    "GROLTS_LABELS_OLD = [\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3a\",\n",
    "    \"3b\",\n",
    "    \"3c\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6a\",\n",
    "    \"6b\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "    \"10\",\n",
    "    \"11\",\n",
    "    \"12\",\n",
    "    \"13\",\n",
    "    \"14a\",\n",
    "    \"14b\",\n",
    "    \"14c\",\n",
    "    \"15\",\n",
    "    \"16\",\n",
    "]\n",
    "GROLTS_LABELS_NEW = [\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "    \"10\",\n",
    "    \"11\",\n",
    "    \"12\",\n",
    "    \"13\",\n",
    "    \"14\",\n",
    "    \"15\",\n",
    "    \"16\",\n",
    "    \"17\",\n",
    "    \"18\",\n",
    "    \"19\",\n",
    "]\n",
    "ID_MAP = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 4,\n",
    "    3: 5,\n",
    "    4: 6,\n",
    "    5: 7,\n",
    "    6: 9,\n",
    "    7: 10,\n",
    "    8: 11,\n",
    "    9: 12,\n",
    "    10: 13,\n",
    "    11: 13,\n",
    "    12: 14,\n",
    "    13: 14,\n",
    "    14: 15,\n",
    "    15: 16,\n",
    "    16: 18,\n",
    "    17: 19,\n",
    "    18: 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_scores(df_labels, files):\n",
    "    \"\"\"\n",
    "    Aggregate total/average scores per paper from each prediction file\n",
    "    and compare them with the true scores from df_labels.\n",
    "    \"\"\"\n",
    "\n",
    "    scores_data = {}\n",
    "\n",
    "    for i, f in enumerate(files):\n",
    "        df = pd.read_csv(f\"{PATH}/{f}\")\n",
    "        # Sum or average the scores per paper (adjust depending on your scale)\n",
    "        paper_scores = df.groupby(\"paper_id\")[\"answer\"].sum()\n",
    "        scores_data[LABELS[i]] = paper_scores\n",
    "\n",
    "    # Combine all models' paper-level scores\n",
    "    scores_df = pd.DataFrame(scores_data).sort_index()\n",
    "\n",
    "    # Compute the true total scores per paper\n",
    "    true_scores = df_labels.groupby(\"paper_id\")[\"answer\"].sum()\n",
    "    scores_df[\"true_score\"] = true_scores\n",
    "    scores_df = scores_df.reset_index()\n",
    "\n",
    "    # Compute rank order per column\n",
    "    rank_df = scores_df.rank(ascending=False, method=\"dense\")\n",
    "    rank_df = rank_df.reset_index()\n",
    "\n",
    "    # Compare rank differences between each model and the true labels\n",
    "    rank_diff = rank_df.subtract(rank_df[\"true_score\"], axis=0)\n",
    "    rank_diff = rank_diff.reset_index()\n",
    "\n",
    "    return scores_df, rank_df, rank_diff\n",
    "\n",
    "\n",
    "def plot_rank_correlation(rank_dfs, labels):\n",
    "    fig, axes = plt.subplots(1, len(rank_dfs), figsize=(6 * len(rank_dfs), 5))\n",
    "    if len(rank_dfs) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, (df, ax) in enumerate(zip(rank_dfs, axes)):\n",
    "        # Select only score columns (exclude 'paper_id' or other metadata)\n",
    "        score_cols = [\n",
    "            c for c in df.columns if c not in [\"paper_id\", \"true_score\", \"index\"]\n",
    "        ]\n",
    "        score_cols.append(\"true_score\")  # include human score for correlation\n",
    "\n",
    "        corr = df[score_cols].corr(method=\"spearman\")\n",
    "        sns.heatmap(corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, ax=ax)\n",
    "        ax.set_title(f\"Rank Correlation - {labels[i]}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_scores_by_paper(scores_df, case_label):\n",
    "    \"\"\"\n",
    "    scores_df: DataFrame with columns ['paper_id', 'true_score', model1, model2, ...]\n",
    "    \"\"\"\n",
    "    # Sort papers by true_score\n",
    "    scores_df = scores_df.sort_values(\"true_score\").reset_index(drop=True)\n",
    "    x = range(len(scores_df))  # numeric positions for plotting\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot true score as a line\n",
    "    plt.plot(\n",
    "        x, scores_df[\"true_score\"], label=\"Human (True)\", color=\"black\", linewidth=2\n",
    "    )\n",
    "\n",
    "    # Plot each LLM as scatter points\n",
    "    llm_cols = [\n",
    "        c for c in scores_df.columns if c not in [\"paper_id\", \"true_score\", \"index\"]\n",
    "    ]\n",
    "    colors = sns.color_palette(\"tab10\", n_colors=len(llm_cols))\n",
    "\n",
    "    for col, color in zip(llm_cols, colors):\n",
    "        plt.scatter(x, scores_df[col], label=col, color=color, s=50, alpha=0.7)\n",
    "\n",
    "    # Set paper_ids as x-tick labels\n",
    "    plt.xticks(x, scores_df[\"paper_id\"].astype(int), rotation=0)\n",
    "\n",
    "    plt.xlabel(\"Paper ID (sorted by human score)\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(f\"Scores per Paper - {case_label}\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def best_matching_llm_rank(scores_df):\n",
    "    llm_cols = [\n",
    "        c for c in scores_df.columns if c not in [\"paper_id\", \"true_score\", \"index\"]\n",
    "    ]\n",
    "    correlations = {}\n",
    "\n",
    "    for col in llm_cols:\n",
    "        corr, _ = spearmanr(scores_df[\"true_score\"], scores_df[col])\n",
    "        correlations[col] = corr\n",
    "\n",
    "    best_llm = max(correlations, key=correlations.get)\n",
    "    return best_llm, correlations\n",
    "\n",
    "\n",
    "def best_matching_llm_mae(scores_df):\n",
    "    llm_cols = [\n",
    "        c for c in scores_df.columns if c not in [\"paper_id\", \"true_score\", \"index\"]\n",
    "    ]\n",
    "    maes = {}\n",
    "\n",
    "    for col in llm_cols:\n",
    "        maes[col] = (scores_df[col] - scores_df[\"true_score\"]).abs().mean()\n",
    "\n",
    "    best_llm = min(maes, key=maes.get)\n",
    "    return best_llm, maes\n",
    "\n",
    "\n",
    "def best_matching_llm_rmse(scores_df):\n",
    "    llm_cols = [\n",
    "        c for c in scores_df.columns if c not in [\"paper_id\", \"true_score\", \"index\"]\n",
    "    ]\n",
    "    rmses = {}\n",
    "\n",
    "    for col in llm_cols:\n",
    "        rmses[col] = np.sqrt(((scores_df[col] - scores_df[\"true_score\"]) ** 2).mean())\n",
    "\n",
    "    best_llm = min(rmses, key=rmses.get)\n",
    "    return best_llm, rmses\n",
    "\n",
    "\n",
    "def ranking_agreement(rank_df, epsilon=2):\n",
    "    \"\"\"\n",
    "    rank_df: DataFrame with columns ['paper_id', 'true_score', model1, model2, ...]\n",
    "             ranks are precomputed (1 = highest)\n",
    "    epsilon: allowed rank difference\n",
    "    Returns: series with fraction of papers within epsilon for each model\n",
    "    \"\"\"\n",
    "    llm_cols = [\n",
    "        c for c in rank_df.columns if c not in [\"paper_id\", \"true_score\", \"index\"]\n",
    "    ]\n",
    "    true_ranks = rank_df[\"true_score\"]\n",
    "    agreement = {}\n",
    "\n",
    "    for col in llm_cols:\n",
    "        diffs = (rank_df[col] - true_ranks).abs()\n",
    "        agreement[col] = (diffs <= epsilon).mean()  # fraction of papers within epsilon\n",
    "\n",
    "    return pd.Series(agreement)\n",
    "\n",
    "\n",
    "def plot_ranking_agreement(all_case_agreements, case_labels):\n",
    "    \"\"\"\n",
    "    all_case_agreements: list of Series from ranking_agreement(), one per case study\n",
    "    case_labels: list of case study names\n",
    "    \"\"\"\n",
    "    df_plot = pd.DataFrame(all_case_agreements, index=case_labels)\n",
    "    df_plot.T.plot(kind=\"bar\", figsize=(10, 6))\n",
    "    plt.ylabel(\"Fraction of papers within epsilon\")\n",
    "    plt.title(\"LLM Ranking Agreement with Human (epsilon method)\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Case Study\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_ranking_scatter_with_epsilon(rank_df, case_label, epsilon=2):\n",
    "    \"\"\"\n",
    "    rank_df: DataFrame with columns ['paper_id', 'true_score', model1, model2, ...]\n",
    "             ranks are precomputed (1 = highest)\n",
    "    epsilon: allowed rank deviation\n",
    "    \"\"\"\n",
    "    # Sort by true ranks for x-axis\n",
    "    rank_df = rank_df.sort_values(\"true_score\").reset_index(drop=True)\n",
    "    x = range(len(rank_df))  # numeric positions for plotting\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # True rank line\n",
    "    plt.plot(x, rank_df[\"true_score\"], label=\"Human Rank\", color=\"black\", linewidth=2)\n",
    "\n",
    "    # Shaded epsilon area\n",
    "    plt.fill_between(\n",
    "        x,\n",
    "        rank_df[\"true_score\"] - epsilon,\n",
    "        rank_df[\"true_score\"] + epsilon,\n",
    "        color=\"gray\",\n",
    "        alpha=0.2,\n",
    "        label=f\"±{epsilon} ranks\",\n",
    "    )\n",
    "\n",
    "    # Plot LLM ranks\n",
    "    llm_cols = [\n",
    "        c for c in rank_df.columns if c not in [\"paper_id\", \"true_score\", \"index\"]\n",
    "    ]\n",
    "    colors = plt.cm.tab10.colors  # up to 10 colors\n",
    "    for col, color in zip(llm_cols, colors):\n",
    "        plt.scatter(x, rank_df[col], label=col, color=color, s=50, alpha=0.7)\n",
    "\n",
    "    # Set paper_ids as x-tick labels\n",
    "    plt.xticks(x, rank_df[\"paper_id\"].astype(int), rotation=0)\n",
    "\n",
    "    plt.xlabel(\"Paper ID (sorted by human rank)\")\n",
    "    plt.ylabel(\"Rank\")\n",
    "    plt.title(f\"LLM Ranking vs Human - {case_label}\")\n",
    "    plt.gca().invert_yaxis()  # rank 1 at top\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f4dd6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './outputs/Qwen_Qwen3-Embedding-8B_gpt-5-mini_wellbeing_500_4.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     13\u001b[39m df_labels[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m] = df_labels[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     15\u001b[39m files = [\n\u001b[32m     16\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQwen_Qwen3-Embedding-8B_gpt-5-mini_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_500_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39msubfolder\u001b[38;5;250m \u001b[39m==\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mptsd\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m4\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQwen_Qwen3-Embedding-8B_microsoft_phi-4_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_500_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39msubfolder\u001b[38;5;250m \u001b[39m==\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mptsd\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m4\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQwen_Qwen3-Embedding-8B_Qwen_Qwen3-Next-80B-A3B-Instruct_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_500_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39msubfolder\u001b[38;5;250m \u001b[39m==\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mptsd\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m4\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m scores_df, rank_df, rank_diff = \u001b[43mload_llm_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# plot_accuracy_with_human(acc_df, subfolder)\u001b[39;00m\n\u001b[32m     26\u001b[39m scores_dfs.append(scores_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mload_llm_scores\u001b[39m\u001b[34m(df_labels, files)\u001b[39m\n\u001b[32m      7\u001b[39m scores_data = {}\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(files):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mf\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Sum or average the scores per paper (adjust depending on your scale)\u001b[39;00m\n\u001b[32m     12\u001b[39m     paper_scores = df.groupby(\u001b[33m\"\u001b[39m\u001b[33mpaper_id\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m].sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/grolts/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/grolts/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/grolts/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/grolts/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/grolts/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './outputs/Qwen_Qwen3-Embedding-8B_gpt-5-mini_wellbeing_500_4.csv'"
     ]
    }
   ],
   "source": [
    "scores_dfs = []\n",
    "rank_dfs = []\n",
    "rank_diffs = []\n",
    "rank_agreements = []\n",
    "# Load labels\n",
    "for subfolder in SUBFOLDERS:\n",
    "    df_labels = pd.read_csv(f\"./human_labels/{subfolder}.csv\", delimiter=\";\", dtype=int)\n",
    "    df_labels = df_labels.melt(\n",
    "        id_vars=[\"paper_id\"], var_name=\"question_id\", value_name=\"answer\"\n",
    "    )\n",
    "    df_labels[\"paper_id\"] = df_labels[\"paper_id\"].astype(int)\n",
    "    df_labels[\"question_id\"] = df_labels[\"question_id\"].astype(int)\n",
    "    df_labels[\"answer\"] = df_labels[\"answer\"].astype(int)\n",
    "\n",
    "    files = [\n",
    "        f\"Qwen_Qwen3-Embedding-8B_gpt-5-mini_{subfolder}_500_{'0' if subfolder == 'ptsd' else '4'}.csv\",\n",
    "        f\"Qwen_Qwen3-Embedding-8B_microsoft_phi-4_{subfolder}_500_{'0' if subfolder == 'ptsd' else '4'}.csv\",\n",
    "        f\"Qwen_Qwen3-Embedding-8B_Qwen_Qwen3-30B-A3B-Instruct-2507_{subfolder}_500_{'0' if subfolder == 'ptsd' else '4'}.csv\",\n",
    "        f\"Qwen_Qwen3-Embedding-8B_meta-llama_Llama-3.3-70B-Instruct_{subfolder}_500_{'0' if subfolder == 'ptsd' else '4'}.csv\",\n",
    "        f\"Qwen_Qwen3-Embedding-8B_mistralai_Magistral-Small-2509_{subfolder}_500_{'0' if subfolder == 'ptsd' else '4'}.csv\",\n",
    "        f\"Qwen_Qwen3-Embedding-8B_Qwen_Qwen3-Next-80B-A3B-Instruct_{subfolder}_500_{'0' if subfolder == 'ptsd' else '4'}.csv\",\n",
    "    ]\n",
    "\n",
    "    scores_df, rank_df, rank_diff = load_llm_scores(df_labels, files)\n",
    "    # plot_accuracy_with_human(acc_df, subfolder)\n",
    "    scores_dfs.append(scores_df)\n",
    "    rank_dfs.append(rank_df)\n",
    "    rank_diffs.append(rank_diff)\n",
    "    plot_scores_by_paper(scores_df, subfolder)\n",
    "    best_llm_sp, correlations = best_matching_llm_rank(scores_df)\n",
    "    print(\n",
    "        f\"Best matching LLM (Spearman) for {subfolder}: {best_llm_sp} with correlations {correlations}\"\n",
    "    )\n",
    "    best_llm_mae, maes = best_matching_llm_mae(scores_df)\n",
    "    print(\n",
    "        f\"Best matching LLM (MAE) for {subfolder}: {best_llm_mae} with correlations {maes}\"\n",
    "    )\n",
    "    best_llm_rmse, rmses = best_matching_llm_rmse(scores_df)\n",
    "    print(\n",
    "        f\"Best matching LLM (RMSE) for {subfolder}: {best_llm_rmse} with correlations {rmses}\"\n",
    "    )\n",
    "\n",
    "    rank_agreements.append(ranking_agreement(rank_df, epsilon=2))\n",
    "    plot_ranking_scatter_with_epsilon(rank_df, subfolder, epsilon=2)\n",
    "\n",
    "\n",
    "plot_rank_correlation(rank_dfs, SUBFOLDERS)\n",
    "plot_ranking_agreement(rank_agreements, SUBFOLDERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grolts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
